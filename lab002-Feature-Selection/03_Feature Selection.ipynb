{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "[1. The needed steps](#1st-bullet)<br>\n",
    "- [1.1 Import the needed libraries](#2nd-bullet)<br>\n",
    "- [1.2 Import the dataset](#3rd-bullet)<br>\n",
    "- [1.3. Data pre-processing, data partition and scaling](#4th-bullet)<br>\n",
    "    \n",
    "[2. Feature Selection](#5th-bullet)<br>\n",
    "- [2.1 Filter methods](#6th-bullet)<br>\n",
    "    - [2.1.1 Univariate variables](#7th-bullet)<br>\n",
    "    - [2.1.2 Spearman Correlation](#8th-bullet)<br>\n",
    "    - [2.1.3 Chi-Square](#9th-bullet)<br>\n",
    "- [2.2 Wrapper Methods](#10th-bullet)<br>\n",
    "    - [2.2.1 RFE](#11th-bullet)<br>\n",
    "- [2.3 Embedded Methods](#12th-bullet)<br>\n",
    "    - [2.3.1 Lasso](#13th-bullet)<br>\n",
    "- [2.4 Final Insights](#14th-bullet)<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"1st-bullet\">    </a>\n",
    "## 1. The needed steps\n",
    "    \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2nd-bullet\">\n",
    "\n",
    "### 1.1. Import the needed libraries\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#filter methods\n",
    "# spearman \n",
    "# chi-square\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "#wrapper methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# embedded methods\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3rd-bullet\">\n",
    "\n",
    "### 1.2. Import the dataset\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ Import the dataset __titanic.csv__ using the method `read_csv()` from pandas. Using the method `head()`, check the first 3 rows of the dataset.<br>\n",
    "- _Documentation pandas.read_csv():_ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html <br>\n",
    "- _Documentation pandas.DataFrame.head():_ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`INPUT VARIABLES`: numerical and categorical <br>\n",
    "`OUPUT VARIABLE`: numeric (binary classification) <br>\n",
    "\n",
    "__GOAL__: Predict survival on the Titanic\n",
    "\n",
    "`Age` : passenger age in years <br>\n",
    "`Cabin` : cabin number <br>\n",
    "`Embarked` : Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton <br>\n",
    "`Fare`: Passenger fare <br>\n",
    "`Name` : Passenger name <br>\n",
    "`Parch`: # of parents / children aboard the Titanic <br>\n",
    "`PassengerId`: Passenger unique identification <br>\n",
    "`Pclass`: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd <br>\n",
    "`Sex`: passenger gender <br>\n",
    "`SibSp`: # of siblings / spouses aboard the Titanic <br>\n",
    "`Survived`: Survival (Dependent variable): 0 = No, 1 = Yes <br>\n",
    "`Ticket`: Ticket number <br>\n",
    "`Title`: Passenger title <br>\n",
    "`Family_Size`: Number of family members onboard <br>\n",
    "`Embark_Hour`: Embark Hour (from 7:00 till 20:00)<br>\n",
    "`Embarked_Flag`: Embark Flag: 0 = No, 1 = Yes<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4th-bullet\">\n",
    "\n",
    "\n",
    "### 1.3. Data pre-processing, data partition and scaling\n",
    "\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ Define the variable `PassengerId` as the new index, using the method `set_index()` from pandas.\n",
    "- _Documentation pandas.DataFrame.set_index():_ https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.set_index('PassengerId', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ Check if you have any missing values in the dataset. You can use the method `info()`from pandas.\n",
    "- _Documentation pandas.DataFrame.info():_ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT - Check the info of the dataset\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the variable \"Cabin\" have only 204 values in the 891 rows - we need to solve this problem of missing values (in step 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__ Define the independent variables as __X__ and the dependent variable ('Survived') as __y__. <br>\n",
    "- _Documentation pandas.DataFrame.drop():_ https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic.drop('Survived', axis = 1)\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ By using the method `train_test_split()` from sklearn.model_selection, split your dataset into train(70%) and validation(30%).<br>\n",
    "- _Documentation sklearn.model_selection.train_test_split():_ https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, random_state = 0, stratify = y, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6`__ There are missing values in the variable 'Cabin'. Fill those with 'Unknown'. <br>\n",
    "- _Documentation pandas.DataFrame.fillna():_ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Cabin'].fillna('Unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_**: You should not use any information from the validation / test dataset to fill the missing values. \n",
    "Let's imagine that you have missing values on age and you want to fill those missing values with the median. You should compute the median value of age for the train dataset (suppose is 35) and fill the missing values in train, validation and test set (if available) with the median in the train dataset (35).\n",
    "\n",
    "__`Step 6B`__ Fill now the missing values in the validation dataset using the same strategy that you did for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "X_val['Cabin'].fillna('Unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ Depending on the feature selection technique used, the input variables can differ - some techniques work only with numerical variables while others with categorical data. Define a new object named a `X_train_num` where only the numerical variables are mantained, and a object named as `X_train_cat` with all the categorical independent variables. Do the same for the validation data.\n",
    "- _Documentation pandas.DataFrame.select_dtypes():_ https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(include=np.number).set_index(X_train.index)\n",
    "X_train_cat = X_train.select_dtypes(exclude=np.number).set_index(X_train.index)\n",
    "# DO IT for validation\n",
    "X_val_num = X_val.select_dtypes(include=np.number).set_index(X_val.index)\n",
    "X_val_cat = X_val.select_dtypes(exclude=np.number).set_index(X_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 8`__ Using the MinMaxScaler from sklearn, scale the numerical data between 0 and 1.\n",
    "- _Documentation sklearn.model_selection.MinMaxScaler():_ https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_num_scaled = scaler.transform(X_train_num) # this will return an array\n",
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_scaled = pd.DataFrame(X_train_num_scaled, columns = X_train_num.columns).set_index(X_train.index)\n",
    "X_train_num_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 8B`__ Using the scaler, scale the numerical validation data also between 0 and 1.\n",
    "\n",
    "**_Note_**: In the same way you did for filling the missing values, the scaling should also consider only the training data. In that way, we are going to use the minimum and maximum values of the training dataset for each variable (the MinMaxScaler use those values to scale the data) to scale also the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_num_scaled = scaler.transform(X_val_num)\n",
    "X_val_num_scaled = pd.DataFrame(X_val_num_scaled, columns = X_val_num.columns).set_index(X_val.index)\n",
    "X_val_num_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"5th-bullet\">    \n",
    "    \n",
    "## 2. Feature Selection\n",
    "    \n",
    "</a>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<a class=\"anchor\" id=\"6th-bullet\">\n",
    "\n",
    "## 2.1. Filter methods\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7th-bullet\">\n",
    "\n",
    "### 2.1.1. Univariate variables\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 9`__ Check if any of the numerical variables is univariate (variance is equal to 0). Drop those variables if existent.\n",
    "- _Documentation pandas.DataFrame.var():_ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.var.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "X_train_num_scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embarked Flag is univariate. We can drop it in the train and in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "X_train_num_scaled.drop('Embarked_Flag', axis = 1, inplace = True)\n",
    "X_val_num_scaled.drop('Embarked_Flag', axis = 1, inplace = True)\n",
    "X_train_num_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8th-bullet\">\n",
    "\n",
    "### 2.1.2. Spearman Correlation\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 10`__ We are going to check now the spearman correlation between the variables. First we need to create a new dataframe with all the training data, but containing also the dependent variable, so we can check if any of the independent variables are correlated with the target. Name this new dataframe as `all_train_num`. \n",
    "- _Documentation pandas.DataFrame.join():_ https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_num = X_train_num_scaled.join(y_train)\n",
    "all_train_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 11`__ Create a function to plot the correlation between the variables named `cor_heatmap()` that should take as parameters the correlation table `cor`. Define the following conditions in the function:\n",
    "- the size of the figure should be (12,10)\n",
    "- create a heatmap using seaborn package where: \n",
    "    - `data` should be equal to the correlation table\n",
    "    - `annot = True`\n",
    "    - `cmap = plt.cm.Reds`\n",
    "    - `fmt='.1'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_heatmap(cor):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(data = cor, annot = True, cmap = plt.cm.Reds, fmt='.1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12`__ Check the spearman correlation of the __all_train__ dataset by applying the method `corr()` and assign it to the object `cor_spearman`\n",
    "- _Documentation pandas.DataFrame.corr():_ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "cor_spearman = all_train_num.corr(method ='spearman')\n",
    "cor_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13`__ Call the function `cor_heatmap()` to the correlation table `cor_spearman`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "cor_heatmap(cor_spearman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that:\n",
    "- There is no independent variable highly correlated with the target. We need to use other feature selection techniques to get more insights.\n",
    "- There are two pairs of variables highly correlated, namely:\n",
    "    - Parch vs Family_Size (0.8) <br>\n",
    "    - SibSp vs Family_Size (0.8) <br>\n",
    "\n",
    "We need to use other techniques of feature selection to decide which variables should we keep (another possible approach was to remove the Family_Size since it is correlated with both variables, but maybe this can have a higher influence on the target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9th-bullet\">\n",
    "\n",
    "### 2.1.3. Chi-Square for categorical data\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to check the importance of the categorical independent variables in the target. For that, we are going to use the dataset `X_train_cat`.\n",
    "\n",
    "__`Step 14`__ Create a function named as `TestIndependence` that should receive as arguments the dataset of independent variables, the target, the name of each independent variable and the alpha defined. This function will follow the steps of chi-square to check if a independent variable is an important predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestIndependence(X,y,var,alpha=0.05):        \n",
    "    dfObserved = pd.crosstab(y,X) \n",
    "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
    "    dfExpected = pd.DataFrame(expected, columns=dfObserved.columns, index = dfObserved.index)\n",
    "    if p<alpha:\n",
    "        result=\"{0} is IMPORTANT for Prediction\".format(var)\n",
    "    else:\n",
    "        result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(var)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 15`__ For all the categorical variables available on `X_train_cat`, call the function `TestIndependence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in X_train_cat:\n",
    "    TestIndependence(X_train_cat[var],y_train, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to chi-square, the only categorical variable that we should discard for the final model is the `Name`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"10th-bullet\">\n",
    "\n",
    "### 2.1.4. Visualize the weight of the dependent variable in categorical data (EXTRA)\n",
    "\n",
    "</a>\n",
    "\n",
    "In this step, we are going to create two plots to check the weight that the dependent variable has in the different labels of a categorical variable. <br><br>\n",
    "The first plot will represent the frequency of each value of the dependent variable in the different categories, and the second plot will make this representation more intuitive by checking the proportion of the presence of each label of the depedent variable in each category. <br><br>\n",
    "Sometimes it can be useful not only to use more advanced techniques like the ones we saw previously, but also to explore visually the variables that we have, like we are going to do in this step.\n",
    "\n",
    "While in the example given you can visualize this relation only to categorical data, you can create your own function where numerical data is split into bins to have a similar visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 16`__ Create a function where you can visualize the proportion of the presence of each label of the dependent variable in each possible value of a categorical feature. Check the \"weight\" of the variable `Sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first join all the training data\n",
    "all_train = X_train.join(y_train)\n",
    "\n",
    "\n",
    "def bar_charts_categorical(df, feature, target):\n",
    "    cont_tab = pd.crosstab(df[feature], df[target], margins = True)\n",
    "    categories = cont_tab.index[:-1]\n",
    "        \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    p1 = plt.bar(categories, cont_tab.iloc[:-1, 0].values, 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, cont_tab.iloc[:-1, 1].values, 0.55, bottom=cont_tab.iloc[:-1, 0], color=\"yellowgreen\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, 0].values, cont_tab.iloc[:-1, 2].values), \n",
    "                        np.divide(cont_tab.iloc[:-1, 1].values, cont_tab.iloc[:-1, 2].values)])\n",
    "      \n",
    "    plt.subplot(122)\n",
    "    p1 = plt.bar(categories, obs_pct[0], 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, obs_pct[1], 0.55, bottom=obs_pct[0], color=\"yellowgreen\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "bar_charts_categorical(all_train, \"Sex\", \"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"11th-bullet\">\n",
    "\n",
    "## 2.2. Wrapper Methods\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12th-bullet\">\n",
    "\n",
    "### 2.2.1. RFE\n",
    "    \n",
    "</a>\n",
    "\n",
    "This time we are going to apply Recursive Feature Elimination (RFE / Backwards) that will allow to select the most important features to keep. The base estimator used will be a Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 17`__ Using LogisticRegression, create a Logistic Regression Classifier instance called `model`\n",
    "- _Documentation sklearn.linear_model.LogisticRegression():_ https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 18`__ Using RFE, create a RFE instance called `rfe` and initialize by defining the following parameters:\n",
    "- `estimator` = model\n",
    "- `n_features_to_select` = 3 <br> <br>\n",
    "\n",
    "- _Documentation sklearn.feature_selection.RFE():_ https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "rfe = RFE(estimator = model, n_features_to_select = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 19`__ Transform the data using RFE by applying the method `fit_transform()` to the instance `rfe`, where `X` should be assigned to your independent variables and `y` to the dependent variable and assign it to the object `X_rfe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "X_rfe = rfe.fit_transform(X = X_train_num_scaled, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 20`__ Check the variables that were selected by RFE as the most \"important\" ones by calling the attribute `support_` to `rfe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 21`__ Check the ranking of the variables defined by RFE as the most \"important\" ones by applying the attribute `ranking_` to `rfe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 22`__  Create an object (pandas Series) named `selected_features` that will have as index the `X_train_num` columns names and the `support_` attribute as value. <br>\n",
    "- _Documentation pandas.Series():_ https://pandas.pydata.org/docs/reference/api/pandas.Series.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = pd.Series(rfe.support_, index = X_train_num_scaled.columns)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 23`__ If you don't know apriori the number of features to select you can create a loop to check the score of the estimator using a different number of features. <br>\n",
    "Below you have an example that will check the score of the estimator by chosing between 1 and 7 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of features\n",
    "nof_list=list(range(1,8))\n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in nof_list:\n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model,n)\n",
    "    X_train_rfe = rfe.fit_transform(X_train_num_scaled,y_train)\n",
    "    X_val_rfe = rfe.transform(X_val_num_scaled)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    \n",
    "    score = model.score(X_val_rfe,y_val)\n",
    "    score_list.append(score)\n",
    "    \n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, and taking into account the results of RFE, we should keep 4 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator = model, n_features_to_select = 4)\n",
    "X_rfe = rfe.fit_transform(X = X_train_num_scaled, y = y_train)\n",
    "selected_features = pd.Series(rfe.support_, index = X_train_num_scaled.columns)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to RFE, the numerical variables that we should keep for the final model are `Age`, `Parch` and `PClass` and `SibSp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"13th-bullet\">\n",
    "\n",
    "## 2.3. Embedded Methods\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"14th-bullet\">\n",
    "\n",
    "### 2.3.1. Lasso Regression\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 24`__ Create a function to plot the importance of the features named `plot_importance()` that should receive as parameters the \"importance\" of each variable (`coef`) and a `name` to define the title.  Define the following conditions/steps in the function:\n",
    "- sort the values of `coef` asn assign it to the object `imp_coef`\n",
    "- the size of the figure should be (8,10)\n",
    "- create a matplotlib plot `kind = barh` to plot the `imp_coef`.\n",
    "- Define the title as \"Feature importance using\" + _the name of the model_ `name` + \" Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(coef,name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 25`__ Using LassoCV, create a Lasso Regression instance called `reg`\n",
    "- _Documentation sklearn.feature_selection.LassoCV():_ https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "reg = LassoCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 26`__ Fit the data to `reg` by using the method `fit()` where `X` should be assigned to `data` and `y` to the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "reg.fit(X_train_num_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 27`__ Create an object (pandas Series) named `coef` that will have as index the `data` columns names and the `coef_` attribute as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "coef = pd.Series(reg.coef_, index = X_train_num_scaled.columns)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 28`__ Check how many variables did Lasso picked (if the coef is different from 0) and how many variables did Lasso eliminated (if the coef is equal to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 29`__ Sort the values of `coef` using the method `sort_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "coef.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 30`__ By calling the function `plot_importance()` that we created on step 15, plot the feature importance of the variables, by passing as parameters the object `coef` and the name `\"Lasso\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "plot_importance(coef,'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Lasso, we should discard the `Family Size`, and the `Embark_hour` seems also insignificant for the definition of the target comparing to the remaining predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"15th-bullet\">\n",
    "\n",
    "## Final Insights:\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Numerical Data\n",
    "\n",
    "| Predictor | Spearman | RFE | Lasso | What to do? (One possible way to \"solve\") | \n",
    "| --- | --- | --- | --- |--- |\n",
    "| Age | Discard | Keep | Keep | Include in the model |\n",
    "| Fare | Discard | Discard | Keep ? | Discard |\n",
    "| Parch | Discard | Keep | Keep ?| Try with and without |\n",
    "| PClass | Discard | Keep | Keep | Include in the model |\n",
    "| SibSp | Discard | Keep | Keep | Include in the model |\n",
    "| Family_Size | Discard | Discard | Discard |  Discard |\n",
    "| Embark_Hour | Discard | Discard | Discard |  Discard |\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "### Categorical Data\n",
    "\n",
    "| Predictor | Chi-Square | \n",
    "| --- | --- | \n",
    "| Cabin | Keep |  \n",
    "| Embarked | Keep | \n",
    "| Name | Discard|\n",
    "| Sex | Keep | \n",
    "| Ticket | Keep | \n",
    "| Title | Keep |\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}