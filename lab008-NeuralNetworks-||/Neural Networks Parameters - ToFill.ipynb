{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Other parameters\n",
    "\n",
    "\n",
    "* [1. The solver](#solver)\n",
    "* [2. The learning rate](#lr)\n",
    "* [3. The learning rate initialization](#lr_init)\n",
    "* [4. The batch size](#batch)\n",
    "* [5. Other parameters](#other)\n",
    "\n",
    "\n",
    "\n",
    "* [Exercise 1. The combination of different ranges in scaling and the use of activation functions](#activation)\n",
    "* [Exercise 2 - The effects of changing the learning rate initialization](#activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous week, we saw that we could change the structure of our network using the parameter `hidden_layer_sizes`, and the number of iterations using `max_iter`. In this notebook we are going to check some other important parameters that can influence the performance of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(model):\n",
    "    # apply kfold\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    # create lists to store the results from the different models \n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    timer = []\n",
    "    n_iter = []\n",
    "    for train_index, test_index in skf.split(X,y):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        min_max = MinMaxScaler().fit(X_train)\n",
    "        # Transform your train data by applying the scale obtained in the previous command\n",
    "        scaled_X_train = min_max.transform(X_train)\n",
    "        # Transform your validation data by applying the scale obtained in the first command\n",
    "        scaled_X_val = min_max.transform(X_val)\n",
    "        \n",
    "        # start counting time\n",
    "        begin = time.perf_counter()\n",
    "        # fit the model to the data\n",
    "        model.fit(scaled_X_train, y_train)\n",
    "        # finish counting time\n",
    "        end = time.perf_counter()\n",
    "        # check the mean accuracy for the train\n",
    "        value_train = model.score(scaled_X_train, y_train)\n",
    "        # check the mean accuracy for the test\n",
    "        value_test = model.score(scaled_X_val,y_val)\n",
    "        # append the accuracies, the time and the number of iterations in the corresponding list\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "        timer.append(end-begin)\n",
    "        n_iter.append(model.n_iter_)\n",
    "    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n",
    "    avg_time = round(np.mean(timer),3)\n",
    "    avg_train = round(np.mean(score_train),3)\n",
    "    avg_test = round(np.mean(score_test),3)\n",
    "    std_time = round(np.std(timer),2)\n",
    "    std_train = round(np.std(score_train),2)\n",
    "    std_test = round(np.std(score_test),2)\n",
    "    avg_iter = round(np.mean(n_iter),1)\n",
    "    std_iter = round(np.std(n_iter),1)\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_train) + '+/-' + str(std_train),\\\n",
    "str(avg_test) + '+/-' + str(std_test), str(avg_iter) + '+/-' + str(std_iter)\n",
    "\n",
    "def show_results(df, *args):\n",
    "    \"\"\"\n",
    "    Receive an empty dataframe and the different models and call the function avg_score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # for each model passed as argument\n",
    "    #scale = scale\n",
    "    for arg in args:\n",
    "        # obtain the results provided by avg_score\n",
    "        time, avg_train, avg_test, avg_iter = avg_score(arg)\n",
    "        # store the results in the right row\n",
    "        df.iloc[count] = time, avg_train, avg_test, avg_iter\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(r'.\\Datasets\\diabetes.csv')\n",
    "X = diabetes.iloc[:,:-1]\n",
    "y = diabetes.iloc[:,-1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.3, random_state = 150, shuffle = True, stratify = y)\n",
    "\n",
    "min_max = MinMaxScaler().fit(X_train)\n",
    "new_X_train = min_max.transform(X_train)\n",
    "new_X_val = min_max.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['Raw'])\n",
    "show_results(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"solver\">\n",
    "    \n",
    "### 1. The solver\n",
    "\n",
    "    \n",
    "</a>\n",
    "\n",
    "#### `default = 'adam'`\n",
    "For more information check this paper: <br>   \n",
    "\n",
    "http://www.robotics.stanford.edu/~ang/papers/icml11-OptimizationForDeepLearning.pdf <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ Create an instance of MLPClassifier, define the solver as __lgfgs__, the maximum number of iterations as __1000__ and name it as __model_lbfgs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lbfgs = MLPClassifier(solver = 'lbfgs', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use__ <br>\n",
    "- L-BFGS is a good option for low dimensional models and for sparse data. <br> Link to L-BFGS paper: http://users.iems.northwestern.edu/~nocedal/PDFfiles/limited-memory.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ Create an instance of MLPClassifier, define the solver as __sgd__, the maximum number of iterations as __1000__ and name it as __model_sgd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = MLPClassifier(solver = 'sgd', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use__\n",
    "- If generalization is more important than time processing - Some recent papers observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance.\n",
    "(https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf)\n",
    "\n",
    "__Notes__\n",
    "- While Gradient Descent use the whole training data to do a single update, in SGD a random data point of the training data to update the parameters - SGD is faster than GD.\n",
    "- It uses a common learning rate for all parameters, contrarialy to what happen in Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ Create an instance of MLPClassifier, define the solver as __adam__, the maximum number of iterations as __1000__ and name it as __model_adam__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = MLPClassifier(solver = 'adam', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use__ <br>\n",
    "- It achieves good results fast - good for complex models, if processing time is an issue. <br>\n",
    "Link to paper: https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "__Notes__ <br>\n",
    "- It computes individual adaptive learning rates for different parameters\n",
    "- Adam combines the advantages of RMSProp and AdaGrad <br>\n",
    "(For more about Adam, check this: https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)\n",
    "- Recent research papers have noted that it can fail to converge to an optimal solution under specific settings.\n",
    "(The paper https://arxiv.org/pdf/1712.07628.pdf demonstrates that adaptive optimization techniques such as Adam generalize poorly compared to SGD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__ Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['lbfgs','sgd','adam'])\n",
    "show_results(df, model_lbfgs, model_sgd, model_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, the number of iterations for __sgd__ and __adam__ correspond to the number of epochs (an epoch consists of one full cycle through the training data.), while for __lbfgs__ it corresponds to one gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"lr\">\n",
    "\n",
    "### 2. The learning rate (Only for sgd)\n",
    "</a>\n",
    "\n",
    "#### `default = 'constant'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ Create an instance of MLPClassifier, define the solver as __sgd__, the learning_rate as __constant__, the maximum number of iterations as __1000__ and name it as __model_constant__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_constant = MLPClassifier(solver = 'sgd', learning_rate = 'constant', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition__<br>\n",
    "If the learning rate is constant, as the name says, the learning rate will always remain equal to the initial learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6`__ Create an instance of MLPClassifier, define the solver as __sgd__, the learning_rate as __invscaling__, the maximum number of iterations as __1000__ and name it as __model_invscaling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_invscaling = MLPClassifier(solver = 'sgd', learning_rate = 'invscaling', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition__<br>\n",
    "If the learning rate is invscaling, it gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. <br><br>\n",
    "$$effective\\; learning\\; rate = \\frac{learning\\_rate\\_init}{t\\;^{power\\_t}}$$ <br>\n",
    "The __power_t__ is another parameter that you can change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ Create an instance of MLPClassifier, define the solver as __sgd__, the learning_rate as __adaptive__, the maximum number of iterations as __1000__ and name it as __model_adaptive__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaptive = MLPClassifier(solver = 'sgd', learning_rate = 'adaptive', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition__ <br>\n",
    "If the learning rate is adaptive, then it keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. <br><br>\n",
    "Each time two consecutive epochs fail to decrease training loss by at least __tol__ (another parameter that you can change), or fail to increase validation score by at least __tol__ if __early_stopping__ (another parameter that you can change) is on, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 8`__ Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['constant','invscaling','adaptive'])\n",
    "show_results(df, model_constant, model_invscaling, model_adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"lr_init\">\n",
    "\n",
    "### 3. The learning rate initialization (only for sgd and adam)\n",
    "    \n",
    "</a>\n",
    "\n",
    "#### `default = '0.001'`\n",
    "\n",
    "The learning rate is one of the most important hyper-parameters to tune for training deep neural networks:\n",
    "\n",
    "__In theory, for the generality of the cases:__ \n",
    "\n",
    "__Small LR__\n",
    "- If the learning rate is small, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny - a smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.\n",
    "- A learning rate that is too small may never converge or may get stuck on a suboptimal solution.\n",
    "\n",
    "__Big LR__\n",
    "- If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse - a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights.\n",
    "\n",
    "\n",
    "The training should start from a relatively large learning rate because, in the beginning, random weights are far from optimal, and then the learning rate can decrease during training to allow more fine-grained weight updates.\n",
    "\n",
    "<img src=\"lr.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 9`__ Create an instance of MLPClassifier, define the solver as __sgd__, the learning_rate_init as __0.5__ and name it as __model_lr_big__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_big = MLPClassifier(learning_rate_init = 0.5, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 10`__ Create an instance of MLPClassifier, define the solver as __sgd__, the learning_rate_init as __0.001__ and name it as __model_lr_medium__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_medium = MLPClassifier(learning_rate_init = 0.001, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 11`__ Create an instance of MLPClassifier, define the solver as __sgd__, the learning_rate_init as __0.000001__ and name it as __model_lr_small__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_small = MLPClassifier(learning_rate_init = 0.000001, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12`__ Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['big','medium','small'])\n",
    "show_results(df, model_lr_big, model_lr_medium, model_lr_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"batch\">\n",
    "\n",
    "### 4. The batch size\n",
    "</a>\n",
    "\n",
    "#### `default = 'auto' (min(200, n_samples))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size can affect significantly the performance and the speed of your training. What happens when you put a batch through your network is that you average the gradients. <br>\n",
    "\n",
    "__In theory, for the generality of the cases:__ \n",
    "\n",
    "__Small batch size__\n",
    "- The lower the batch size, the higher the probability of your estimate being less accurate, since the networks weights can \"jump\" around if your data is noisy, and it might be unable to learn, or it converges very slow. Besides that, the computation time is going to increase.\n",
    "- It can be useful in some cases to escape local minima.\n",
    "- Sometimes, and depending on your computational resources, this is the only option.\n",
    "\n",
    "__Big batch size__\n",
    "- If your batch size is big enough, this will provide a stable enough estimate of what the gradient of the full dataset would be, since you will have fewer gradient updates per epoch.\n",
    "- In the same logic, it is desired to speed up computation, due to a lower quantity of updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13`__ Create an instance of MLPClassifier, define the batch_size as __1__ and name it as __model_batch1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch1 = MLPClassifier(batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 14`__ Create an instance of MLPClassifier, define the batch_size as __50__ and name it as __model_batch50__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch50 = MLPClassifier(batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 15`__ Create an instance of MLPClassifier, define the batch_size equal to the number of samples we have in the training dataset and name it as __model_batchlen__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batchlen = MLPClassifier(batch_size = len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 16`__ Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['batch 1','batch 50','batch ' + str(len(X_train))])\n",
    "show_results(df, model_batch1, model_batch50, model_batchlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model_batch1 = MLPClassifier(solver = 'sgd', batch_size = 1, random_state = 20).fit(new_X_train, y_train)\n",
    "model_batch50 = MLPClassifier(solver = 'sgd', batch_size = 50, random_state = 20).fit(new_X_train, y_train)\n",
    "model_batchlen = MLPClassifier(solver = 'sgd', batch_size = len(X_train), random_state = 20).fit(new_X_train, y_train)\n",
    "\n",
    "losses_batch1 = model_batch1.loss_curve_\n",
    "iterations_batch1 = range(model_batch1.n_iter_)\n",
    "losses_batch50 = model_batch50.loss_curve_\n",
    "iterations_batch50 = range(model_batch50.n_iter_)\n",
    "losses_batchlen = model_batchlen.loss_curve_\n",
    "iterations_batchlen = range(model_batchlen.n_iter_)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(131)\n",
    "plt.title('batch1 | Final Loss: ' + str(round(model_batch1.loss_,2)), fontsize=20)\n",
    "sns.lineplot(iterations_batch1, losses_batch1)\n",
    "plt.subplot(132)\n",
    "plt.title('batch50 | Final Loss: ' + str(round(model_batch50.loss_,2)),fontsize=20)\n",
    "sns.lineplot(iterations_batch50, losses_batch50)\n",
    "plt.subplot(133)\n",
    "plt.title('batch' + str(len(X_train)) + ' | Final Loss: ' + str(round(model_batchlen.loss_,2)),fontsize=20)\n",
    "sns.lineplot(iterations_batchlen, losses_batchlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"other\">\n",
    "\n",
    "### 5. Other parameters\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter| Definition | LBFGS | SGD | ADAM |\n",
    "|---|---|---|---|---|\n",
    "|alpha| L2 penalty (regularization term) parameter | yes | yes | yes |\n",
    "| power_t | The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’. | no | yes | no |\n",
    "| shuffle | Whether to shuffle samples in each iteration. | no | yes | yes |\n",
    "| tol | Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops. | yes | yes | yes |\n",
    "| warm_start | When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. | yes | yes | yes |\n",
    "| momentum | Momentum for gradient descent update. Should be between 0 and 1. | no | yes | no |\n",
    "| nesterovs_momentum | Whether to use Nesterov’s momentum.| no | yes | no |\n",
    "| early stopping | Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs. The split is stratified, except in a multilabel setting.  | no | yes | yes |\n",
    "| validation_fraction | The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True | no | yes | yes|\n",
    "| beta1 | Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). | no | no | yes |\n",
    "| beta2 | Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1).  | no | no | yes |\n",
    "| epsilon | Value for numerical stability in adam. | no | no | yes |\n",
    "| n_iter_no_change | Maximum number of epochs to not meet tol improvement. |  no | yes | yes |\n",
    "| max_fun | Only used when solver=’lbfgs’. Maximum number of loss function calls. The solver iterates until convergence (determined by ‘tol’), number of iterations reaches max_iter, or this number of loss function calls. | yes | no | no |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">><font color='Orange'> __Practice__  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Exercise 1 - The combination of different ranges in scaling and the use of activation functions\n",
    "    \n",
    "</div>\n",
    "\n",
    "<a class=\"anchor\" id=\"activation\">\n",
    "    \n",
    "### The activation function \n",
    "</a>\n",
    "\n",
    "#### `default = 'relu'`\n",
    "\n",
    "Check this link for more information regarding the advantages and disadvantages of different activation functions: <br>https://indiantechwarrior.com/7-types-of-neural-network-activation-functions-how-to-choose/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ Create an instance of MLPClassifier, define the activation as __relu__, the __hidden_layer_sizes = (8)__ and the __max_iter = 1000__. Name it as __model_relu__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - __Advantages:__\n",
    "     - Computationally efficient - allows the network to converge very quickly.\n",
    "     - Nonlinear - Although it looks like a linear function, ReLU has a derivative function and allows for backpropagation\n",
    " - __Disadvantages:__\n",
    "     - The dying ReLU problem - When inputs approach zero, or are negative, the gradient of the function becomes zero and the network cannot perform backpropagation and cannot learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ Create an instance of MLPClassifier, define the activation as __logistic__, the __hidden_layer_sizes = (8)__ and the __max_iter = 1000__. Name it as __model_logistic__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - __Advantages:__\n",
    "     - Smooth gradient, preventing “jumps” in output values.\n",
    "     - Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    " - __Disadvantages:__\n",
    "     - Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or have slow convergence.\n",
    "     - Computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ Create a new avg_score function, named as `avg_score2`, but this time include the option of applying also MinMaxScaler in the range [-1,1].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameter 'scale' in the function avg_score2 is a string that will receive \n",
    "# the option 'minmax' (range between 0 and 1) or minmax2 (range between -1 and 1) \n",
    "def avg_score2(model, scale): \n",
    "    # write your function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results2(df, scale, *args):\n",
    "    \"\"\"\n",
    "    Receive an empty dataframe and the different models and call the function avg_score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # for each model passed as argument\n",
    "    scale = scale\n",
    "    for arg in args:\n",
    "        # obtain the results provided by avg_score\n",
    "        time, avg_train, avg_test, avg_iter = avg_score2(arg, scale)\n",
    "        # store the results in the right row\n",
    "        df.iloc[count] = time, avg_train, avg_test, avg_iter\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__Using the function `show_results2`, check what is the average number of iterations needed to find convergence using the activation functions relu, logistic and tanh, using MinMaxScaler in the range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ Check the weights of the connections between the first neuron in the input layer and the hidden layer by using the attribute __coefs___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6`__Using the function `show_results2`, check what is the average number of iterations needed to find convergence using the activation functions relu, logistic and tanh, using MinMaxScaler in the range [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ Check now the weights of the connections between the first neuron in the input layer and the hidden layer by using the attribute __coefs___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Exercise 2 - The effects of changing the learning rate initialization\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this exercise, you are going to check how changing the initialization of the learning rate can lead to distinct loss curves during the training of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ Create three models:\n",
    "- `model_lr_big` - An MLPClassifier, with `solver = 'sgd'`, `random_state = 5` and `learning_rate_init = 0.5`;\n",
    "- `model_lr_medium` - An MLPClassifier, with `solver = 'sgd'`, `random_state = 5` and `learning_rate_init = 0.001`;\n",
    "- `model_lr_small` - An MLPClassifier, with `solver = 'sgd'`, `random_state = 5` and `learning_rate_init = 0.000001`;\n",
    "\n",
    "Fit all your models to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ Make three plots where you visualize the loss curve for each of the models, and check the final loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
